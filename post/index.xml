<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | NMI</title>
    <link>https://eneftci.github.io/post/</link>
      <atom:link href="https://eneftci.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 06 Dec 2020 17:21:59 -0700</lastBuildDate>
    <image>
      <url>https://eneftci.github.io/images/logo_hu7945e16b4c080225a93098c1772fbb4a_6934_300x300_fit_box_2.png</url>
      <title>Posts</title>
      <link>https://eneftci.github.io/post/</link>
    </image>
    
    <item>
      <title>New Article Proceedings of the IEEE on Brain-Inspired Learning on Neuromorphic Substrates</title>
      <link>https://eneftci.github.io/post/zenke_neftci20/</link>
      <pubDate>Sun, 06 Dec 2020 17:21:59 -0700</pubDate>
      <guid>https://eneftci.github.io/post/zenke_neftci20/</guid>
      <description>&lt;p&gt;Neuromorphic hardware strives to emulate brain-like neural networks and thus holds the promise for scalable, low-power information processing on temporal data streams. Yet, to solve real-world problems, these networks need to be trained. However, training on neuromorphic substrates creates significant challenges due to the offline character and the required nonlocal computations of gradient-based learning algorithms. This article provides a mathematical framework for the design of practical online learning algorithms for neuromorphic substrates. Specifically, we show a direct connection between RTRL, an online algorithm for computing gradients in conventional RNNs, and biologically plausible learning rules for training SNNs. Furthermore, we motivate a sparse approximation based on block-diagonal Jacobians, which reduces the algorithm&amp;rsquo;s computational complexity, diminishes the nonlocal information requirements, and empirically leads to good learning performance, thereby improving its applicability to neuromorphic substrates. In summary, our framework bridges the gap between synaptic plasticity and gradient-based approaches from deep learning and lays the foundations for powerful information processing on future neuromorphic hardware systems.&lt;/p&gt;
&lt;p&gt;See &lt;a href=../../publication/zenke-neftci-21/ &gt;publication here &lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New Article: Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)</title>
      <link>https://eneftci.github.io/post/kaiser_etal20_frontiers/</link>
      <pubDate>Tue, 12 May 2020 17:21:59 -0700</pubDate>
      <guid>https://eneftci.github.io/post/kaiser_etal20_frontiers/</guid>
      <description>&lt;p&gt;A growing body of work underlines striking similarities between biological neural networks and recurrent, binary neural networks. A relatively smaller body of work, however, addresses the similarities between learning dynamics employed in deep artificial neural networks and synaptic plasticity in spiking neural networks. The challenge preventing this is largely caused by the discrepancy between the dynamical properties of synaptic plasticity and the requirements for gradient backpropagation. Learning algorithms that approximate gradient backpropagation using local error functions can overcome this challenge. Here, we introduce Deep Continuous Local Learning (DECOLLE), a spiking neural network equipped with local error functions for online learning with no memory overhead for computing gradients. DECOLLE is capable of learning deep spatio temporal representations from spikes relying solely on local information, making it compatible with neurobiology and neuromorphic hardware. Synaptic plasticity rules are derived systematically from user-defined cost functions and neural dynamics by leveraging existing autodifferentiation methods of machine learning frameworks. We benchmark our approach on the event-based neuromorphic dataset N-MNIST and DvsGesture, on which DECOLLE performs comparably to the state-of-the-art. DECOLLE networks provide continuously learning machines that are relevant to biology and supportive of event-based, low-power computer vision architectures matching the accuracies of conventional computers on tasks where temporal precision and speed are essential.&lt;/p&gt;
&lt;p&gt;See &lt;a href=&#34;https://www.frontiersin.org/articles/10.3389/fnins.2020.00424/full&#34; &gt;publication here &lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New Article NeurIPS 2019 on Stochastic Neural Networks</title>
      <link>https://eneftci.github.io/post/detorakis_etal19_neurips/</link>
      <pubDate>Fri, 13 Mar 2020 17:21:59 -0700</pubDate>
      <guid>https://eneftci.github.io/post/detorakis_etal19_neurips/</guid>
      <description>&lt;p&gt;Multiplicative stochasticity such as Dropout improves the robustness and gener- alizability deep neural networks. Here, we further demonstrate that always-on multiplicative stochasticity combined with simple threshold neurons provide a suf- ficient substrate for deep learning machines. We call such models Neural Sampling Machines (NSM). We find that the probability of activation of the NSM exhibits a self-normalizing property that mirrors Weight Normalization, a previously studied mechanism that fulfills many of the features of Batch Normalization in an online fashion. The normalization of activities during training speeds up convergence by preventing internal covariate shift caused by changes in the distribution of inputs. The always-on stochasticity of the NSM confers the following advantages: the network is identical in the inference and learning phases, making the NSM a suitable substrate for continual learning, it can exploit stochasticity inherent to a physical substrate such as analog non-volatile memories for in memory computing, and it is suitable for Monte Carlo sampling, while requiring almost exclusively addition and comparison operations. We demonstrate NSMs on standard classification benchmarks (MNIST and CIFAR) and event-based classification benchmarks (N-MNIST and DVS Gestures). Our results show that NSMs perform comparably or better than conventional artificial neural networks with the same architecture.&lt;/p&gt;
&lt;p&gt;See &lt;a href=../../publication/detorakis-etal-19-a/ &gt;publication here &lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New Article: Signal Processing Magazine on Surrogate Gradient Learning</title>
      <link>https://eneftci.github.io/post/neftci_etal19_spm/</link>
      <pubDate>Fri, 13 Mar 2020 17:21:59 -0700</pubDate>
      <guid>https://eneftci.github.io/post/neftci_etal19_spm/</guid>
      <description>&lt;p&gt;Spiking neural networks (SNNs) are nature&amp;rsquo;s versatile solution to fault-tolerant, energy-efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking NN processors have attempted to emulate biological NNs. These developments have created an imminent need for methods and tools that enable such systems to solve real-world signal processing problems. Like conventional NNs, SNNs can be trained on real, domain-specific data; however, their training requires the overcoming of a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training SNNs and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. Accordingly, it gives an overview of existing approaches and provides an introduction to surrogate gradient (SG) methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.&lt;/p&gt;
&lt;p&gt;See &lt;a href=../../publication/neftci-etal-19/ &gt;publication here &lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
